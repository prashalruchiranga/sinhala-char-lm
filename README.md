## Overview

This code can be used to train and sample from character-level language models using Recurrent Neural Networks. The underlying model takes Sinhala text as input and trains a Gated Recurrent Unit (GRU) that learns to predict the next character in the sequence. Unlike English, the Sinhala language contains not only characters but also a rich set of diacritics, ligatures, and conjunct forms. Therefore, language modeling is more complex and requires careful Unicode handling.

## Requirements

The code is written in Python 3.11 and requires TensorFlow 2.18. Refer to the `pyproject.toml` file for additional dependencies. To set up the project, follow the steps described below.

### MacOS / Linux

Clone the repository
```
$ git clone https://github.com/prashalruchiranga/sinhala-char-lm.git
$ cd sinhala-char-lm
```
Install `uv` package manager. For more details on installing on other operating systems, visit the [official uv documentation](https://docs.astral.sh/uv/getting-started/installation/).
```
# Using Homebrew for macOS
$ brew install uv

# or
$ curl -LsSf https://astral.sh/uv/install.sh | sh
```

Create and activate virtual environment.
```
$ uv venv --python=python3.11
$ source .venv/bin/activate
```

Install development dependencies.
```
$ uv sync
```

## Usage

I trained the models on a dataset of excerpts from Ada Derana Sinhala articles. This dataset is more than enough for next character prediction with RNNs. The dataset is available at [9wimu9/ada_derana_sinhala](https://huggingface.co/datasets/9wimu9/ada_derana_sinhala) on Hugging Face. Note that the data cleaning logic defined in this code may require modification if you choose to use a different dataset. Alternatively, you can provide a pre-cleaned text file as input for training.

To begin training, simply run `train.py`. All configuration parameters are defined in the `config.yaml` file. The default Hugging Face dataset used here is approximately 159 MB in size. You may need a GPU or TPU to train efficiently with the full dataset. With a `sequence_length` of 200 and a `batch_size` of 512, a single TPU v3 with 8 cores took approximately 2 minutes per epoch. If you do not wish to use the full training set, you can adjust the `training_frac` parameter to a value between 0 and 1 to use a subset of the dataset.
```
$ python3 train.py
```

If you already have a cleaned text file, provide its path as a command-line argument.
```
$ python3 train.py --file "/path/to/the/cleaned/text/file.txt"
```

Run `sample.py` to sample from the trained model. Text is generated by iteratively running the trained model for a number of steps specified by the `sample_size` parameter. You can control the generation with `initial_input` and `temperature` arguments to get different results.
```
$ python3 sample.py --string "මැතිවරණ" --length 100 --temperature 0.8
```

## Acknowledgements

This work is largely inspired by Andrej Karpathy's [char-rnn](https://github.com/karpathy/char-rnn), in which he implements multi-layer Recurrent Neural Networks (RNN, LSTM, and GRU) for training and sampling from character-level language models. Refer to his [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) for background and context on training RNNs for next-character prediction tasks.

## License

Licensed under MIT. See the [LICENSE](https://github.com/prashalruchiranga/sinhala-char-lm/blob/main/LICENSE).
